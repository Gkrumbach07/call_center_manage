{"ast":null,"code":"const Long = require('../../utils/long');\n\nconst Decoder = require('../decoder');\n\nconst MessageDecoder = require('../message/decoder');\n\nconst {\n  lookupCodecByAttributes\n} = require('../message/compression');\n\nconst {\n  KafkaJSPartialMessageError\n} = require('../../errors');\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\n\nmodule.exports = async (primaryDecoder, size = null) => {\n  const messages = [];\n  const messageSetSize = size || primaryDecoder.readInt32();\n  const messageSetDecoder = primaryDecoder.slice(messageSetSize);\n\n  while (messageSetDecoder.offset < messageSetSize) {\n    try {\n      const message = EntryDecoder(messageSetDecoder);\n      const codec = lookupCodecByAttributes(message.attributes);\n\n      if (codec) {\n        const buffer = await codec.decompress(message.value);\n        messages.push(...EntriesDecoder(new Decoder(buffer), message));\n      } else {\n        messages.push(message);\n      }\n    } catch (e) {\n      if (e.name === 'KafkaJSPartialMessageError') {\n        // We tried to decode a partial message, it means that minBytes\n        // is probably too low\n        break;\n      }\n\n      if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n        // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n        // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n        // receive the full record batch on the next request\n        break;\n      }\n\n      throw e;\n    }\n  }\n\n  primaryDecoder.forward(messageSetSize);\n  return messages;\n};\n\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = [];\n\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder));\n  }\n\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset);\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset);\n    const baseOffset = compressedOffset - lastMessageOffset;\n\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset).add(baseOffset).toString();\n    }\n  }\n\n  return messages;\n};\n\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(`Tried to decode a partial message: There isn't enough bytes to read the offset`);\n  }\n\n  const offset = decoder.readInt64().toString();\n\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(`Tried to decode a partial message: There isn't enough bytes to read the message size`);\n  }\n\n  const size = decoder.readInt32();\n  return MessageDecoder(offset, size, decoder);\n};","map":{"version":3,"sources":["/Users/gagekrumbach/Documents/call-center-manage/node_modules/kafkajs/src/protocol/messageSet/decoder.js"],"names":["Long","require","Decoder","MessageDecoder","lookupCodecByAttributes","KafkaJSPartialMessageError","module","exports","primaryDecoder","size","messages","messageSetSize","readInt32","messageSetDecoder","slice","offset","message","EntryDecoder","codec","attributes","buffer","decompress","value","push","EntriesDecoder","e","name","forward","decoder","compressedMessage","length","magicByte","compressedOffset","fromValue","lastMessageOffset","baseOffset","add","toString","canReadInt64","readInt64","canReadInt32"],"mappings":"AAAA,MAAMA,IAAI,GAAGC,OAAO,CAAC,kBAAD,CAApB;;AACA,MAAMC,OAAO,GAAGD,OAAO,CAAC,YAAD,CAAvB;;AACA,MAAME,cAAc,GAAGF,OAAO,CAAC,oBAAD,CAA9B;;AACA,MAAM;AAAEG,EAAAA;AAAF,IAA8BH,OAAO,CAAC,wBAAD,CAA3C;;AACA,MAAM;AAAEI,EAAAA;AAAF,IAAiCJ,OAAO,CAAC,cAAD,CAA9C;AAEA;AACA;AACA;AACA;AACA;AACA;;;AAEAK,MAAM,CAACC,OAAP,GAAiB,OAAOC,cAAP,EAAuBC,IAAI,GAAG,IAA9B,KAAuC;AACtD,QAAMC,QAAQ,GAAG,EAAjB;AACA,QAAMC,cAAc,GAAGF,IAAI,IAAID,cAAc,CAACI,SAAf,EAA/B;AACA,QAAMC,iBAAiB,GAAGL,cAAc,CAACM,KAAf,CAAqBH,cAArB,CAA1B;;AAEA,SAAOE,iBAAiB,CAACE,MAAlB,GAA2BJ,cAAlC,EAAkD;AAChD,QAAI;AACF,YAAMK,OAAO,GAAGC,YAAY,CAACJ,iBAAD,CAA5B;AACA,YAAMK,KAAK,GAAGd,uBAAuB,CAACY,OAAO,CAACG,UAAT,CAArC;;AAEA,UAAID,KAAJ,EAAW;AACT,cAAME,MAAM,GAAG,MAAMF,KAAK,CAACG,UAAN,CAAiBL,OAAO,CAACM,KAAzB,CAArB;AACAZ,QAAAA,QAAQ,CAACa,IAAT,CAAc,GAAGC,cAAc,CAAC,IAAItB,OAAJ,CAAYkB,MAAZ,CAAD,EAAsBJ,OAAtB,CAA/B;AACD,OAHD,MAGO;AACLN,QAAAA,QAAQ,CAACa,IAAT,CAAcP,OAAd;AACD;AACF,KAVD,CAUE,OAAOS,CAAP,EAAU;AACV,UAAIA,CAAC,CAACC,IAAF,KAAW,4BAAf,EAA6C;AAC3C;AACA;AACA;AACD;;AAED,UAAID,CAAC,CAACC,IAAF,KAAW,yCAAf,EAA0D;AACxD;AACA;AACA;AACA;AACD;;AAED,YAAMD,CAAN;AACD;AACF;;AAEDjB,EAAAA,cAAc,CAACmB,OAAf,CAAuBhB,cAAvB;AACA,SAAOD,QAAP;AACD,CApCD;;AAsCA,MAAMc,cAAc,GAAG,CAACI,OAAD,EAAUC,iBAAV,KAAgC;AACrD,QAAMnB,QAAQ,GAAG,EAAjB;;AAEA,SAAOkB,OAAO,CAACb,MAAR,GAAiBa,OAAO,CAACR,MAAR,CAAeU,MAAvC,EAA+C;AAC7CpB,IAAAA,QAAQ,CAACa,IAAT,CAAcN,YAAY,CAACW,OAAD,CAA1B;AACD;;AAED,MAAIC,iBAAiB,CAACE,SAAlB,GAA8B,CAA9B,IAAmCF,iBAAiB,CAACd,MAAlB,IAA4B,CAAnE,EAAsE;AACpE,UAAMiB,gBAAgB,GAAGhC,IAAI,CAACiC,SAAL,CAAeJ,iBAAiB,CAACd,MAAjC,CAAzB;AACA,UAAMmB,iBAAiB,GAAGlC,IAAI,CAACiC,SAAL,CAAevB,QAAQ,CAACA,QAAQ,CAACoB,MAAT,GAAkB,CAAnB,CAAR,CAA8Bf,MAA7C,CAA1B;AACA,UAAMoB,UAAU,GAAGH,gBAAgB,GAAGE,iBAAtC;;AAEA,SAAK,MAAMlB,OAAX,IAAsBN,QAAtB,EAAgC;AAC9BM,MAAAA,OAAO,CAACD,MAAR,GAAiBf,IAAI,CAACiC,SAAL,CAAejB,OAAO,CAACD,MAAvB,EACdqB,GADc,CACVD,UADU,EAEdE,QAFc,EAAjB;AAGD;AACF;;AAED,SAAO3B,QAAP;AACD,CApBD;;AAsBA,MAAMO,YAAY,GAAGW,OAAO,IAAI;AAC9B,MAAI,CAACA,OAAO,CAACU,YAAR,EAAL,EAA6B;AAC3B,UAAM,IAAIjC,0BAAJ,CACH,gFADG,CAAN;AAGD;;AAED,QAAMU,MAAM,GAAGa,OAAO,CAACW,SAAR,GAAoBF,QAApB,EAAf;;AAEA,MAAI,CAACT,OAAO,CAACY,YAAR,EAAL,EAA6B;AAC3B,UAAM,IAAInC,0BAAJ,CACH,sFADG,CAAN;AAGD;;AAED,QAAMI,IAAI,GAAGmB,OAAO,CAAChB,SAAR,EAAb;AACA,SAAOT,cAAc,CAACY,MAAD,EAASN,IAAT,EAAemB,OAAf,CAArB;AACD,CAjBD","sourcesContent":["const Long = require('../../utils/long')\nconst Decoder = require('../decoder')\nconst MessageDecoder = require('../message/decoder')\nconst { lookupCodecByAttributes } = require('../message/compression')\nconst { KafkaJSPartialMessageError } = require('../../errors')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\nmodule.exports = async (primaryDecoder, size = null) => {\n  const messages = []\n  const messageSetSize = size || primaryDecoder.readInt32()\n  const messageSetDecoder = primaryDecoder.slice(messageSetSize)\n\n  while (messageSetDecoder.offset < messageSetSize) {\n    try {\n      const message = EntryDecoder(messageSetDecoder)\n      const codec = lookupCodecByAttributes(message.attributes)\n\n      if (codec) {\n        const buffer = await codec.decompress(message.value)\n        messages.push(...EntriesDecoder(new Decoder(buffer), message))\n      } else {\n        messages.push(message)\n      }\n    } catch (e) {\n      if (e.name === 'KafkaJSPartialMessageError') {\n        // We tried to decode a partial message, it means that minBytes\n        // is probably too low\n        break\n      }\n\n      if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n        // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n        // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n        // receive the full record batch on the next request\n        break\n      }\n\n      throw e\n    }\n  }\n\n  primaryDecoder.forward(messageSetSize)\n  return messages\n}\n\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = []\n\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder))\n  }\n\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset)\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset)\n    const baseOffset = compressedOffset - lastMessageOffset\n\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset)\n        .add(baseOffset)\n        .toString()\n    }\n  }\n\n  return messages\n}\n\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the offset`\n    )\n  }\n\n  const offset = decoder.readInt64().toString()\n\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the message size`\n    )\n  }\n\n  const size = decoder.readInt32()\n  return MessageDecoder(offset, size, decoder)\n}\n"]},"metadata":{},"sourceType":"script"}
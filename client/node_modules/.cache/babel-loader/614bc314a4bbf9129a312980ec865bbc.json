{"ast":null,"code":"const Decoder = require('../../decoder');\n\nconst {\n  KafkaJSPartialMessageError\n} = require('../../../errors');\n\nconst {\n  lookupCodecByAttributes\n} = require('../../message/compression');\n\nconst RecordDecoder = require('../record/v0/decoder');\n\nconst TimestampTypes = require('../../timestampTypes');\n\nconst TIMESTAMP_TYPE_FLAG_MASK = 0x8;\nconst TRANSACTIONAL_FLAG_MASK = 0x10;\nconst CONTROL_FLAG_MASK = 0x20;\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nmodule.exports = async fetchDecoder => {\n  const firstOffset = fetchDecoder.readInt64().toString();\n  const length = fetchDecoder.readInt32();\n  const decoder = fetchDecoder.slice(length);\n  fetchDecoder.forward(length);\n  const remainingBytes = Buffer.byteLength(decoder.buffer);\n\n  if (remainingBytes < length) {\n    throw new KafkaJSPartialMessageError(`Tried to decode a partial record batch: remainingBytes(${remainingBytes}) < recordBatchLength(${length})`);\n  }\n\n  const partitionLeaderEpoch = decoder.readInt32(); // The magic byte was read by the Fetch protocol to distinguish between\n  // the record batch and the legacy message set. It's not used here but\n  // it has to be read.\n\n  const magicByte = decoder.readInt8(); // eslint-disable-line no-unused-vars\n  // The library is currently not performing CRC validations\n\n  const crc = decoder.readInt32(); // eslint-disable-line no-unused-vars\n\n  const attributes = decoder.readInt16();\n  const lastOffsetDelta = decoder.readInt32();\n  const firstTimestamp = decoder.readInt64().toString();\n  const maxTimestamp = decoder.readInt64().toString();\n  const producerId = decoder.readInt64().toString();\n  const producerEpoch = decoder.readInt16();\n  const firstSequence = decoder.readInt32();\n  const inTransaction = (attributes & TRANSACTIONAL_FLAG_MASK) > 0;\n  const isControlBatch = (attributes & CONTROL_FLAG_MASK) > 0;\n  const timestampType = (attributes & TIMESTAMP_TYPE_FLAG_MASK) > 0 ? TimestampTypes.LOG_APPEND_TIME : TimestampTypes.CREATE_TIME;\n  const codec = lookupCodecByAttributes(attributes);\n  const recordContext = {\n    firstOffset,\n    firstTimestamp,\n    partitionLeaderEpoch,\n    inTransaction,\n    isControlBatch,\n    lastOffsetDelta,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    maxTimestamp,\n    timestampType\n  };\n  const records = await decodeRecords(codec, decoder, { ...recordContext,\n    magicByte\n  });\n  return { ...recordContext,\n    records\n  };\n};\n\nconst decodeRecords = async (codec, recordsDecoder, recordContext) => {\n  if (!codec) {\n    return recordsDecoder.readArray(decoder => decodeRecord(decoder, recordContext));\n  }\n\n  const length = recordsDecoder.readInt32();\n\n  if (length <= 0) {\n    return [];\n  }\n\n  const compressedRecordsBuffer = recordsDecoder.readAll();\n  const decompressedRecordBuffer = await codec.decompress(compressedRecordsBuffer);\n  const decompressedRecordDecoder = new Decoder(decompressedRecordBuffer);\n  const records = new Array(length);\n\n  for (let i = 0; i < length; i++) {\n    records[i] = decodeRecord(decompressedRecordDecoder, recordContext);\n  }\n\n  return records;\n};\n\nconst decodeRecord = (decoder, recordContext) => {\n  const recordBuffer = decoder.readVarIntBytes();\n  return RecordDecoder(new Decoder(recordBuffer), recordContext);\n};","map":{"version":3,"sources":["/Users/gagekrumbach/Documents/call-center-manage/node_modules/kafkajs/src/protocol/recordBatch/v0/decoder.js"],"names":["Decoder","require","KafkaJSPartialMessageError","lookupCodecByAttributes","RecordDecoder","TimestampTypes","TIMESTAMP_TYPE_FLAG_MASK","TRANSACTIONAL_FLAG_MASK","CONTROL_FLAG_MASK","module","exports","fetchDecoder","firstOffset","readInt64","toString","length","readInt32","decoder","slice","forward","remainingBytes","Buffer","byteLength","buffer","partitionLeaderEpoch","magicByte","readInt8","crc","attributes","readInt16","lastOffsetDelta","firstTimestamp","maxTimestamp","producerId","producerEpoch","firstSequence","inTransaction","isControlBatch","timestampType","LOG_APPEND_TIME","CREATE_TIME","codec","recordContext","records","decodeRecords","recordsDecoder","readArray","decodeRecord","compressedRecordsBuffer","readAll","decompressedRecordBuffer","decompress","decompressedRecordDecoder","Array","i","recordBuffer","readVarIntBytes"],"mappings":"AAAA,MAAMA,OAAO,GAAGC,OAAO,CAAC,eAAD,CAAvB;;AACA,MAAM;AAAEC,EAAAA;AAAF,IAAiCD,OAAO,CAAC,iBAAD,CAA9C;;AACA,MAAM;AAAEE,EAAAA;AAAF,IAA8BF,OAAO,CAAC,2BAAD,CAA3C;;AACA,MAAMG,aAAa,GAAGH,OAAO,CAAC,sBAAD,CAA7B;;AACA,MAAMI,cAAc,GAAGJ,OAAO,CAAC,sBAAD,CAA9B;;AAEA,MAAMK,wBAAwB,GAAG,GAAjC;AACA,MAAMC,uBAAuB,GAAG,IAAhC;AACA,MAAMC,iBAAiB,GAAG,IAA1B;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEAC,MAAM,CAACC,OAAP,GAAiB,MAAMC,YAAN,IAAsB;AACrC,QAAMC,WAAW,GAAGD,YAAY,CAACE,SAAb,GAAyBC,QAAzB,EAApB;AACA,QAAMC,MAAM,GAAGJ,YAAY,CAACK,SAAb,EAAf;AACA,QAAMC,OAAO,GAAGN,YAAY,CAACO,KAAb,CAAmBH,MAAnB,CAAhB;AACAJ,EAAAA,YAAY,CAACQ,OAAb,CAAqBJ,MAArB;AAEA,QAAMK,cAAc,GAAGC,MAAM,CAACC,UAAP,CAAkBL,OAAO,CAACM,MAA1B,CAAvB;;AAEA,MAAIH,cAAc,GAAGL,MAArB,EAA6B;AAC3B,UAAM,IAAIb,0BAAJ,CACH,0DAAyDkB,cAAe,yBAAwBL,MAAO,GADpG,CAAN;AAGD;;AAED,QAAMS,oBAAoB,GAAGP,OAAO,CAACD,SAAR,EAA7B,CAdqC,CAgBrC;AACA;AACA;;AACA,QAAMS,SAAS,GAAGR,OAAO,CAACS,QAAR,EAAlB,CAnBqC,CAmBA;AAErC;;AACA,QAAMC,GAAG,GAAGV,OAAO,CAACD,SAAR,EAAZ,CAtBqC,CAsBL;;AAEhC,QAAMY,UAAU,GAAGX,OAAO,CAACY,SAAR,EAAnB;AACA,QAAMC,eAAe,GAAGb,OAAO,CAACD,SAAR,EAAxB;AACA,QAAMe,cAAc,GAAGd,OAAO,CAACJ,SAAR,GAAoBC,QAApB,EAAvB;AACA,QAAMkB,YAAY,GAAGf,OAAO,CAACJ,SAAR,GAAoBC,QAApB,EAArB;AACA,QAAMmB,UAAU,GAAGhB,OAAO,CAACJ,SAAR,GAAoBC,QAApB,EAAnB;AACA,QAAMoB,aAAa,GAAGjB,OAAO,CAACY,SAAR,EAAtB;AACA,QAAMM,aAAa,GAAGlB,OAAO,CAACD,SAAR,EAAtB;AAEA,QAAMoB,aAAa,GAAG,CAACR,UAAU,GAAGrB,uBAAd,IAAyC,CAA/D;AACA,QAAM8B,cAAc,GAAG,CAACT,UAAU,GAAGpB,iBAAd,IAAmC,CAA1D;AACA,QAAM8B,aAAa,GACjB,CAACV,UAAU,GAAGtB,wBAAd,IAA0C,CAA1C,GACID,cAAc,CAACkC,eADnB,GAEIlC,cAAc,CAACmC,WAHrB;AAKA,QAAMC,KAAK,GAAGtC,uBAAuB,CAACyB,UAAD,CAArC;AAEA,QAAMc,aAAa,GAAG;AACpB9B,IAAAA,WADoB;AAEpBmB,IAAAA,cAFoB;AAGpBP,IAAAA,oBAHoB;AAIpBY,IAAAA,aAJoB;AAKpBC,IAAAA,cALoB;AAMpBP,IAAAA,eANoB;AAOpBG,IAAAA,UAPoB;AAQpBC,IAAAA,aARoB;AASpBC,IAAAA,aAToB;AAUpBH,IAAAA,YAVoB;AAWpBM,IAAAA;AAXoB,GAAtB;AAcA,QAAMK,OAAO,GAAG,MAAMC,aAAa,CAACH,KAAD,EAAQxB,OAAR,EAAiB,EAAE,GAAGyB,aAAL;AAAoBjB,IAAAA;AAApB,GAAjB,CAAnC;AAEA,SAAO,EACL,GAAGiB,aADE;AAELC,IAAAA;AAFK,GAAP;AAID,CA7DD;;AA+DA,MAAMC,aAAa,GAAG,OAAOH,KAAP,EAAcI,cAAd,EAA8BH,aAA9B,KAAgD;AACpE,MAAI,CAACD,KAAL,EAAY;AACV,WAAOI,cAAc,CAACC,SAAf,CAAyB7B,OAAO,IAAI8B,YAAY,CAAC9B,OAAD,EAAUyB,aAAV,CAAhD,CAAP;AACD;;AAED,QAAM3B,MAAM,GAAG8B,cAAc,CAAC7B,SAAf,EAAf;;AAEA,MAAID,MAAM,IAAI,CAAd,EAAiB;AACf,WAAO,EAAP;AACD;;AAED,QAAMiC,uBAAuB,GAAGH,cAAc,CAACI,OAAf,EAAhC;AACA,QAAMC,wBAAwB,GAAG,MAAMT,KAAK,CAACU,UAAN,CAAiBH,uBAAjB,CAAvC;AACA,QAAMI,yBAAyB,GAAG,IAAIpD,OAAJ,CAAYkD,wBAAZ,CAAlC;AACA,QAAMP,OAAO,GAAG,IAAIU,KAAJ,CAAUtC,MAAV,CAAhB;;AAEA,OAAK,IAAIuC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGvC,MAApB,EAA4BuC,CAAC,EAA7B,EAAiC;AAC/BX,IAAAA,OAAO,CAACW,CAAD,CAAP,GAAaP,YAAY,CAACK,yBAAD,EAA4BV,aAA5B,CAAzB;AACD;;AAED,SAAOC,OAAP;AACD,CArBD;;AAuBA,MAAMI,YAAY,GAAG,CAAC9B,OAAD,EAAUyB,aAAV,KAA4B;AAC/C,QAAMa,YAAY,GAAGtC,OAAO,CAACuC,eAAR,EAArB;AACA,SAAOpD,aAAa,CAAC,IAAIJ,OAAJ,CAAYuD,YAAZ,CAAD,EAA4Bb,aAA5B,CAApB;AACD,CAHD","sourcesContent":["const Decoder = require('../../decoder')\nconst { KafkaJSPartialMessageError } = require('../../../errors')\nconst { lookupCodecByAttributes } = require('../../message/compression')\nconst RecordDecoder = require('../record/v0/decoder')\nconst TimestampTypes = require('../../timestampTypes')\n\nconst TIMESTAMP_TYPE_FLAG_MASK = 0x8\nconst TRANSACTIONAL_FLAG_MASK = 0x10\nconst CONTROL_FLAG_MASK = 0x20\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nmodule.exports = async fetchDecoder => {\n  const firstOffset = fetchDecoder.readInt64().toString()\n  const length = fetchDecoder.readInt32()\n  const decoder = fetchDecoder.slice(length)\n  fetchDecoder.forward(length)\n\n  const remainingBytes = Buffer.byteLength(decoder.buffer)\n\n  if (remainingBytes < length) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial record batch: remainingBytes(${remainingBytes}) < recordBatchLength(${length})`\n    )\n  }\n\n  const partitionLeaderEpoch = decoder.readInt32()\n\n  // The magic byte was read by the Fetch protocol to distinguish between\n  // the record batch and the legacy message set. It's not used here but\n  // it has to be read.\n  const magicByte = decoder.readInt8() // eslint-disable-line no-unused-vars\n\n  // The library is currently not performing CRC validations\n  const crc = decoder.readInt32() // eslint-disable-line no-unused-vars\n\n  const attributes = decoder.readInt16()\n  const lastOffsetDelta = decoder.readInt32()\n  const firstTimestamp = decoder.readInt64().toString()\n  const maxTimestamp = decoder.readInt64().toString()\n  const producerId = decoder.readInt64().toString()\n  const producerEpoch = decoder.readInt16()\n  const firstSequence = decoder.readInt32()\n\n  const inTransaction = (attributes & TRANSACTIONAL_FLAG_MASK) > 0\n  const isControlBatch = (attributes & CONTROL_FLAG_MASK) > 0\n  const timestampType =\n    (attributes & TIMESTAMP_TYPE_FLAG_MASK) > 0\n      ? TimestampTypes.LOG_APPEND_TIME\n      : TimestampTypes.CREATE_TIME\n\n  const codec = lookupCodecByAttributes(attributes)\n\n  const recordContext = {\n    firstOffset,\n    firstTimestamp,\n    partitionLeaderEpoch,\n    inTransaction,\n    isControlBatch,\n    lastOffsetDelta,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    maxTimestamp,\n    timestampType,\n  }\n\n  const records = await decodeRecords(codec, decoder, { ...recordContext, magicByte })\n\n  return {\n    ...recordContext,\n    records,\n  }\n}\n\nconst decodeRecords = async (codec, recordsDecoder, recordContext) => {\n  if (!codec) {\n    return recordsDecoder.readArray(decoder => decodeRecord(decoder, recordContext))\n  }\n\n  const length = recordsDecoder.readInt32()\n\n  if (length <= 0) {\n    return []\n  }\n\n  const compressedRecordsBuffer = recordsDecoder.readAll()\n  const decompressedRecordBuffer = await codec.decompress(compressedRecordsBuffer)\n  const decompressedRecordDecoder = new Decoder(decompressedRecordBuffer)\n  const records = new Array(length)\n\n  for (let i = 0; i < length; i++) {\n    records[i] = decodeRecord(decompressedRecordDecoder, recordContext)\n  }\n\n  return records\n}\n\nconst decodeRecord = (decoder, recordContext) => {\n  const recordBuffer = decoder.readVarIntBytes()\n  return RecordDecoder(new Decoder(recordBuffer), recordContext)\n}\n"]},"metadata":{},"sourceType":"script"}
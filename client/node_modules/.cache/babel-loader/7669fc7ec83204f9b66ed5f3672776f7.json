{"ast":null,"code":"const Decoder = require('../../../decoder');\n\nconst MessageSetDecoder = require('../../../messageSet/decoder');\n\nconst RecordBatchDecoder = require('../../../recordBatch/v0/decoder');\n\nconst {\n  MAGIC_BYTE\n} = require('../../../recordBatch/v0'); // the magic offset is at the same offset for all current message formats, but the 4 bytes\n// between the size and the magic is dependent on the version.\n\n\nconst MAGIC_OFFSET = 16;\nconst RECORD_BATCH_OVERHEAD = 49;\n\nconst decodeMessages = async decoder => {\n  const messagesSize = decoder.readInt32();\n\n  if (messagesSize <= 0 || !decoder.canReadBytes(messagesSize)) {\n    return [];\n  }\n\n  const messagesBuffer = decoder.readBytes(messagesSize);\n  const messagesDecoder = new Decoder(messagesBuffer);\n  const magicByte = messagesBuffer.slice(MAGIC_OFFSET).readInt8(0);\n\n  if (magicByte === MAGIC_BYTE) {\n    let records = [];\n\n    while (messagesDecoder.canReadBytes(RECORD_BATCH_OVERHEAD)) {\n      try {\n        const recordBatch = await RecordBatchDecoder(messagesDecoder);\n        records = [...records, ...recordBatch.records];\n      } catch (e) {\n        // The tail of the record batches can have incomplete records\n        // due to how maxBytes works. See https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-FetchAPI\n        if (e.name === 'KafkaJSPartialMessageError') {\n          break;\n        }\n\n        throw e;\n      }\n    }\n\n    return records;\n  }\n\n  return MessageSetDecoder(messagesDecoder, messagesSize);\n};\n\nmodule.exports = decodeMessages;","map":{"version":3,"sources":["/Users/gagekrumbach/Documents/call-center-manage/node_modules/kafkajs/src/protocol/requests/fetch/v4/decodeMessages.js"],"names":["Decoder","require","MessageSetDecoder","RecordBatchDecoder","MAGIC_BYTE","MAGIC_OFFSET","RECORD_BATCH_OVERHEAD","decodeMessages","decoder","messagesSize","readInt32","canReadBytes","messagesBuffer","readBytes","messagesDecoder","magicByte","slice","readInt8","records","recordBatch","e","name","module","exports"],"mappings":"AAAA,MAAMA,OAAO,GAAGC,OAAO,CAAC,kBAAD,CAAvB;;AACA,MAAMC,iBAAiB,GAAGD,OAAO,CAAC,6BAAD,CAAjC;;AACA,MAAME,kBAAkB,GAAGF,OAAO,CAAC,iCAAD,CAAlC;;AACA,MAAM;AAAEG,EAAAA;AAAF,IAAiBH,OAAO,CAAC,yBAAD,CAA9B,C,CAEA;AACA;;;AACA,MAAMI,YAAY,GAAG,EAArB;AACA,MAAMC,qBAAqB,GAAG,EAA9B;;AAEA,MAAMC,cAAc,GAAG,MAAMC,OAAN,IAAiB;AACtC,QAAMC,YAAY,GAAGD,OAAO,CAACE,SAAR,EAArB;;AAEA,MAAID,YAAY,IAAI,CAAhB,IAAqB,CAACD,OAAO,CAACG,YAAR,CAAqBF,YAArB,CAA1B,EAA8D;AAC5D,WAAO,EAAP;AACD;;AAED,QAAMG,cAAc,GAAGJ,OAAO,CAACK,SAAR,CAAkBJ,YAAlB,CAAvB;AACA,QAAMK,eAAe,GAAG,IAAId,OAAJ,CAAYY,cAAZ,CAAxB;AACA,QAAMG,SAAS,GAAGH,cAAc,CAACI,KAAf,CAAqBX,YAArB,EAAmCY,QAAnC,CAA4C,CAA5C,CAAlB;;AAEA,MAAIF,SAAS,KAAKX,UAAlB,EAA8B;AAC5B,QAAIc,OAAO,GAAG,EAAd;;AAEA,WAAOJ,eAAe,CAACH,YAAhB,CAA6BL,qBAA7B,CAAP,EAA4D;AAC1D,UAAI;AACF,cAAMa,WAAW,GAAG,MAAMhB,kBAAkB,CAACW,eAAD,CAA5C;AACAI,QAAAA,OAAO,GAAG,CAAC,GAAGA,OAAJ,EAAa,GAAGC,WAAW,CAACD,OAA5B,CAAV;AACD,OAHD,CAGE,OAAOE,CAAP,EAAU;AACV;AACA;AACA,YAAIA,CAAC,CAACC,IAAF,KAAW,4BAAf,EAA6C;AAC3C;AACD;;AAED,cAAMD,CAAN;AACD;AACF;;AAED,WAAOF,OAAP;AACD;;AAED,SAAOhB,iBAAiB,CAACY,eAAD,EAAkBL,YAAlB,CAAxB;AACD,CAjCD;;AAmCAa,MAAM,CAACC,OAAP,GAAiBhB,cAAjB","sourcesContent":["const Decoder = require('../../../decoder')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\nconst RecordBatchDecoder = require('../../../recordBatch/v0/decoder')\nconst { MAGIC_BYTE } = require('../../../recordBatch/v0')\n\n// the magic offset is at the same offset for all current message formats, but the 4 bytes\n// between the size and the magic is dependent on the version.\nconst MAGIC_OFFSET = 16\nconst RECORD_BATCH_OVERHEAD = 49\n\nconst decodeMessages = async decoder => {\n  const messagesSize = decoder.readInt32()\n\n  if (messagesSize <= 0 || !decoder.canReadBytes(messagesSize)) {\n    return []\n  }\n\n  const messagesBuffer = decoder.readBytes(messagesSize)\n  const messagesDecoder = new Decoder(messagesBuffer)\n  const magicByte = messagesBuffer.slice(MAGIC_OFFSET).readInt8(0)\n\n  if (magicByte === MAGIC_BYTE) {\n    let records = []\n\n    while (messagesDecoder.canReadBytes(RECORD_BATCH_OVERHEAD)) {\n      try {\n        const recordBatch = await RecordBatchDecoder(messagesDecoder)\n        records = [...records, ...recordBatch.records]\n      } catch (e) {\n        // The tail of the record batches can have incomplete records\n        // due to how maxBytes works. See https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-FetchAPI\n        if (e.name === 'KafkaJSPartialMessageError') {\n          break\n        }\n\n        throw e\n      }\n    }\n\n    return records\n  }\n\n  return MessageSetDecoder(messagesDecoder, messagesSize)\n}\n\nmodule.exports = decodeMessages\n"]},"metadata":{},"sourceType":"script"}